{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4bccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.12.0\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting cachetools==4.2.1\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting certifi==2020.12.5\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting chardet==4.0.0\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting cmake==3.18.4.post1\n",
      "  Using cached cmake-3.18.4.post1-py3-none-manylinux1_x86_64.whl (17.7 MB)\n",
      "Collecting decorator==4.4.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting facenet-pytorch==2.5.1\n",
      "  Using cached facenet_pytorch-2.5.1-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: flatbuffers==1.12 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.12)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-auth==1.28.1\n",
      "  Using cached google_auth-1.28.1-py2.py3-none-any.whl (136 kB)\n",
      "Collecting google-auth-oauthlib==0.4.4\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-pasta==0.2.0\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio==1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting idna==2.10\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting imageio==2.9.0\n",
      "  Using cached imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "Collecting imageio-ffmpeg==0.4.3\n",
      "  Using cached imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "Requirement already satisfied: imutils==0.5.4 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (0.5.4)\n",
      "Collecting Markdown==3.3.4\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting moviepy==1.0.3\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting mtcnn==0.1.0\n",
      "  Using cached mtcnn-0.1.0-py3-none-any.whl (2.3 MB)\n",
      "Requirement already satisfied: numpy==1.19.5 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (1.19.5)\n",
      "Collecting oauthlib==3.1.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting opencv-python==4.5.1.48\n",
      "  Using cached opencv_python-4.5.1.48-cp38-cp38-manylinux2014_x86_64.whl (50.4 MB)\n",
      "Collecting opt-einsum==3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting Pillow==8.2.0\n",
      "  Using cached Pillow-8.2.0-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting proglog==0.1.9\n",
      "  Using cached proglog-0.1.9-py3-none-any.whl\n",
      "Collecting protobuf==3.15.8\n",
      "  Using cached protobuf-3.15.8-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (0.4.8)\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting PyYAML==5.4.1\n",
      "  Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "Collecting requests==2.25.1\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting requests-oauthlib==1.3.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa==4.7.2\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting scipy==1.6.2\n",
      "  Using cached scipy-1.6.2-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
      "Collecting six==1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard==2.4.1\n",
      "  Using cached tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (1.8.0)\n",
      "Collecting tensorflow==2.4.1\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "Requirement already satisfied: tensorflow-estimator==2.4.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (2.4.0)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (1.1.0)\n",
      "Collecting torch==1.8.1\n",
      "  Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "Collecting torchvision==0.9.1\n",
      "  Using cached torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
      "Collecting tqdm==4.60.0\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (3.7.4.3)\n",
      "Collecting urllib3==1.26.4\n",
      "  Using cached urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting Werkzeug==1.0.1\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Requirement already satisfied: wrapt==1.12.1 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from astunparse==1.6.3->-r requirements.txt (line 2)) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from google-auth==1.28.1->-r requirements.txt (line 12)) (59.8.0)\n",
      "Requirement already satisfied: keras>=2.0.0 in /home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages (from mtcnn==0.1.0->-r requirements.txt (line 23)) (2.10.0)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Installing collected packages: cmake, certifi, Werkzeug, urllib3, tqdm, torch, six, scipy, rsa, PyYAML, pyasn1-modules, Pillow, opt-einsum, opencv-python, oauthlib, Markdown, imageio-ffmpeg, idna, gast, decorator, chardet, cachetools, torchvision, requests, protobuf, proglog, mtcnn, keras-preprocessing, imageio, h5py, grpcio, google-pasta, google-auth, astunparse, absl-py, requests-oauthlib, moviepy, facenet-pytorch, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.6.15.1\n",
      "    Uninstalling certifi-2022.6.15.1:\n",
      "      Successfully uninstalled certifi-2022.6.15.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.11\n",
      "    Uninstalling urllib3-1.26.11:\n",
      "      Successfully uninstalled urllib3-1.26.11\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.9.1\n",
      "    Uninstalling scipy-1.9.1:\n",
      "      Successfully uninstalled scipy-1.9.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.6.0a0+82fd1c8\n",
      "    Uninstalling torchvision-0.6.0a0+82fd1c8:\n",
      "      Successfully uninstalled torchvision-0.6.0a0+82fd1c8\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Markdown-3.3.4 Pillow-8.2.0 PyYAML-5.4.1 Werkzeug-1.0.1 absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 certifi-2020.12.5 chardet-4.0.0 cmake-3.18.4.post1 decorator-4.4.2 facenet-pytorch-2.5.1 gast-0.3.3 google-auth-1.28.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 imageio-2.9.0 imageio-ffmpeg-0.4.3 keras-preprocessing-1.1.2 moviepy-1.0.3 mtcnn-0.1.0 oauthlib-3.1.0 opencv-python-4.5.1.48 opt-einsum-3.3.0 proglog-0.1.9 protobuf-3.15.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.2 six-1.15.0 tensorboard-2.4.1 tensorflow-2.4.1 torch-1.8.1 torchvision-0.9.1 tqdm-4.60.0 urllib3-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa878",
   "metadata": {},
   "source": [
    "# 1. Crosscut Class 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b9620",
   "metadata": {},
   "source": [
    "### - 1. 초기화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f39c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, dist_obj, video_path, output_path):\n",
    "    # 무대 영상들이 저장된 폴더 경로\n",
    "    self.videos_path = video_path\n",
    "    # 만든 교차편집 영상을 저장할 경로와 파일명\n",
    "    self.output_path = output_path\n",
    "    # 무대 영상 중 가장 짧은 영상의 길이(초)로 클래스 내부에서 자동 계산\n",
    "    # 1000.0 : 초기값\n",
    "    self.min_time = 1000.0\n",
    "    # 무대 영상 개수\n",
    "    video_num = len(os.listdir(self.videos_path))\n",
    "    # 각 무대 영상의 시작점\n",
    "    self.start_times = [0] * video_num\n",
    "    # 영상을 비교할 길이(초)\n",
    "    self.window_time = 10\n",
    "    # 영상 전환이 너무 자주 일어나면 영상이 산만해질 수 있으므로\n",
    "    # 일정 시간 동안 전환을 방지\n",
    "    self.padded_time = 4\n",
    "    # Random거리, 얼굴기반 Landmark, 자세 기반 Landmark 중 하나를 정해 할당\n",
    "    self.dist_obj = dist_obj\n",
    "    # 교차편집에 사용할 음성\n",
    "    self.audioclip = None\n",
    "    # 시작점이 정렬된 무대 영상 저장\n",
    "    self.extracted_clips_array = []\n",
    "    # 교차편집으로 내보낼 짧은 영상들 저장\n",
    "    self.con_clips = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee89922",
   "metadata": {},
   "source": [
    "### - 2. 무대 영상들의 시작점을 정렬하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701055cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_alignment(self):\n",
    "    for i in range(len(os.listdir(self.videos_path))):\n",
    "        video_path = os.path.join(self.videos_path, \n",
    "                                 sorted(so.listdir(self.videos_path))[i])\n",
    "        clip = VidoeFileClip(video_path)\n",
    "        clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "        \n",
    "        if self.min_time > clip.duration:\n",
    "            self.audioclip = clip.audio\n",
    "            self.min_time = clip.duration\n",
    "        self.extracted_clips_array.append(clip)\n",
    "    print('LOGGER-- {} Vidoe Will Be Mixed'.format (len(self.extracted_clips_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f9686",
   "metadata": {},
   "source": [
    "### - 3. 다음 영상을 선택하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cac977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_clip(sefl, t, current_idx):\n",
    "    # 1. 거리 측정에 필요한 변수 초기화하기\n",
    "    cur_t = t\n",
    "    next_t = min(t + self.window_time, self.min_time)\n",
    "    \n",
    "    reference_clip = self.extracted_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "    d = float('Inf')\n",
    "    cur_clip = None\n",
    "    min_idx = (current_idx + 1) % len(self.extracted_clips_array)\n",
    "    \n",
    "    # 2. 비교 영상들 과 현재 영상의 거리 측정\n",
    "    for video_idx in range(len(self.extracted_clips_array)):\n",
    "        if video_idx == current_idx:\n",
    "            continue\n",
    "        clip = self.extracted_clips_array[video_idx].subclip(cur_t, next_t)\n",
    "        cur_d, plus_frame = self.dist_obj.distance(reference_clip, clip)\n",
    "        print(current_idx, video_idx,cur_d, cur_t + plus_frame)\n",
    "        if d > cur_d:\n",
    "            d = cur_d\n",
    "            min_idx = video_idx\n",
    "            next_t = cur_t + plus_frame\n",
    "            cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "            \n",
    "    # 3. 다음 교차편집 지점 전까지 현재 영상 저장하기\n",
    "    if cur_clip:\n",
    "        clip = cur_clip\n",
    "    else:\n",
    "        clip = reference_clip\n",
    "    self.con_clips.append(clip)\n",
    "    \n",
    "    # 4. 현재 시간을 갱신하고 다음에 사용할 영상 인덱스 반환하기\n",
    "    t = next_t\n",
    "    return t, min_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd28696",
   "metadata": {},
   "source": [
    "### - 4. 선택한 영상의 padding을 추가하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b50d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(sef, t, next_idx):\n",
    "    print('idx : {}'.format(next_idx))\n",
    "    pad_clip = self.extracted_clips_arra[next_idx].subclip(t,min(self.min_time, t + self.padded_time))\n",
    "    self.con_clips.append(pad_clip)\n",
    "    \n",
    "    t = min(self.min_time, t + self.padded_time)\n",
    "    return t, next_idx    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1edb1e3",
   "metadata": {},
   "source": [
    "### - 5. 교차편집을 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6019584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(self):\n",
    "    final_clip = concatenate_videoclips(self.con_clips)\n",
    "    if self.audioclip != None:\n",
    "        print(\"Not None\")\n",
    "        final_clip.audio= self.audioclip\n",
    "    final_clip.write_videofile(self.output_path)\n",
    "    return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffea49",
   "metadata": {},
   "source": [
    "### - 6. 교차편집을 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc88461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(self):\n",
    "    # 1. 영상 전처리하기\n",
    "    self.video_alignment()\n",
    "    t = 3\n",
    "    corrent_idx = 0\n",
    "    self.con_clips.append(self.extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time))))\n",
    "    \n",
    "    # 2. 노래 끝까지 교차편집 영상 만들기\n",
    "    while t < int(self.min_time):\n",
    "        t, min_idx = self.select_next_clip(t, current_idx)\n",
    "        t, current_idx = self.add_padding(t,min_idx)\n",
    "        \n",
    "    # 3. 교차편집 결과 영상 저장하기\n",
    "    final_clip = self.write_video()\n",
    "    return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b115a72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcc6407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8d8ac7",
   "metadata": {},
   "source": [
    "### - 7. 랜덤으로 거리와 시간을 생성하는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e98524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDistance:\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        dur_end = min(reference_clip.duration, compare_clip.duration)\n",
    "        return random.randrange(1, 100), min(dur_end, random.randrange(3, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cd562",
   "metadata": {},
   "source": [
    "# 2. FaceDistance Class 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ba77d",
   "metadata": {},
   "source": [
    "### - 1. 초기화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e18741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, shape_predictor_path, face_embedding_penalty = None):\n",
    "    # skip_frame_rate : 두 영상 간의 거리를 계산할 Frame 단위\n",
    "    self.skip_frame_rate = 4\n",
    "    # 얼굴이 가장 비슷한 지점에서 영상 전환\n",
    "    self. minimax_frames = 5\n",
    "    # shape_predictor_68_face_landmarks.dat 모델 경로\n",
    "    self.shape_predictor = shape_predictor_path\n",
    "    # face_embedding_penalty : 두 영상에서 얼굴이 다를 때 더해지는 최대 penalty 값\n",
    "    self.face_embedding_penalty = face_embedding_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894632e",
   "metadata": {},
   "source": [
    "### - 2. 얼굴의 Landmarks를 추출하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399d5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark(self, reference_clip, compare_clip):\n",
    "    # 1. 영상 저장 및 face landmark detect model 불러오기\n",
    "    # 비교할 두 영상을 self.clips에 저장\n",
    "    self.clips = [reference_clip, compare_clip]\n",
    "    # 얼굴 자체의 위치를 사각형으로 잡아주는 모델\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    # 얼굴 위치가 잡힌 사각형 내에서 얼굴의 landmark를 구해주는 모델\n",
    "    predictor = dlib.shape_predictor(self.shape_predictor)\n",
    "    clips_frame_info = []\n",
    "    \n",
    "    for clip in self.clips:\n",
    "        # 2. 각 영상의 정보를 저장하기 위해 loop마다 초기화하기\n",
    "        # 현재 확인중인 frame 번호. 현재 frame이 몇 번째인지 알려줌\n",
    "        i = 0\n",
    "        # 각 영상의 Frame 정보 담기\n",
    "        every_frame_info = []\n",
    "        \n",
    "        while True:\n",
    "            # 3. 각 영상에서 face Landmark 얻기\n",
    "            # get_frame : 해당 시간의 Frame을 이미지로 바꿔서 Frame에 반환하는 Moviepy 제공 함수\n",
    "            frame = clip.get_frame(i * 1.0 / clip.fps)\n",
    "            # 다음 loop에서 사용할 frame index 저장\n",
    "            i += self.skip_frame_rate\n",
    "            # 영상의 끝부분까지 Facial Landmark를 얻기 위한 while문\n",
    "            # clip.fps : 1초당 들어 있는 Frame 수\n",
    "            # 1.0 / clip.fps : 한 Frame이 차지하는 시간\n",
    "            # i * 1.0 . clip.fps : 현재 확인하고 있는 Frame의 시간대\n",
    "            if(i*1.0/clip.fps) > clip.duration:\n",
    "                break\n",
    "            \n",
    "            # 이미지가 작을수록 계산 속도가 빠르나 정확도가 낮아짐\n",
    "            frame = imutils.resize(frame, width = 800)\n",
    "            # 흑백사진을 이용하면 계산 속도가 빨라짐\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # detector로 얼굴의 위치를 사각형으로 잡아줌\n",
    "            rects = detector(gray, 0)\n",
    "            \n",
    "            # 4. 얻은 face Landmark를 가공해서 every_frame_info에 저장\n",
    "            # 얼굴 개수 파악. 하나라도 있으면 facial landmark 계산, 없으면 빈 list 반환\n",
    "            if len(rects) > 0:\n",
    "                max_width = 0\n",
    "                #계산의 효율성을 위해 얼굴 중에 가장 큰 얼굴만 detect\n",
    "                max_rect = None\n",
    "                # max_width를 통해 가장 너비가 큰 사각형을 찾고, max_rect에 해당 얼굴 저장\n",
    "                for rect in rects:\n",
    "                    if int(rect[0].width()) > max_width:\n",
    "                        max_rect = rect\n",
    "                # 가장 크게 확대된 얼굴의 facial landmark를 구함\n",
    "                shape = predictor(gray, max_rect)\n",
    "                # 구해진 facial landmark 결과를 계산 용이성을 위해 numpy로 변환\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "                # 해당 frame의 facial landmark 정보를 every_frame_info에 추가\n",
    "                every_frame_info.append(shape)\n",
    "            else:\n",
    "                every_frame_info.append([])\n",
    "                \n",
    "        # 5. 영상 frame별 landmark 정보를 clips_frame_info에 저장\n",
    "        # 영상의 모든 frame의 facial landmark 정보가 담긴 every_frame_info를 clips_frame_info에 저장\n",
    "        clips_frame_info.append(np.array(every_frame_info))\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "    return clips_frame_info                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70541dcb",
   "metadata": {},
   "source": [
    "### - 3. 얼굴의 Embedding 값의 Cosine Distance를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf37874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 Embedding 값으로 penalty를 주는 함수\n",
    "def embedding_cosine_distance(self, reference_frame, compare_frame):\n",
    "    face_detector = MTCNN(select_largest = True)\n",
    "    embed_model = InceptionResnetV1(pretrained = 'vggface2').eval()\n",
    "    \n",
    "    reference_frame = np.array(reference_frame)\n",
    "    compare_frame = np.array(compare_frame)\n",
    "    try :\n",
    "        reference_frame_detected = face_detector(reference_frame)\n",
    "        compare_frame_detected = face_detector(compare_frame)\n",
    "    except:\n",
    "        cosine_dist = 1\n",
    "        return cosine_dist\n",
    "    \n",
    "    reference_frame_embed = embed_model(reference_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "    compare_frame_embed = embed_model(compare_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "    reference_frame_embed = np.squeeze(reference_frame_embed)\n",
    "    compare_frame_embed = np.squeeze(compare_frame_embed)\n",
    "    cosine_dist = 1 - np.dot(reference_frame_embed, compare_frame_embed) / (np.linalg.nrom(reference_frame_embed) * np.linalg.norm(compare_frame_embed))\n",
    "\n",
    "    return cosine_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3faf83",
   "metadata": {},
   "source": [
    "### - 4. 두 영상의 각 Frame 간 거리를 측정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378fb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "    # dist_arr에는 같은 시간에 두 영상의 거리를 각 frame별로 저장\n",
    "    dist_arr = []\n",
    "    # clip_frame_info에서 각 영상이 가진 frame 정보 순회\n",
    "    for i in range(min_size - 1):\n",
    "        # 두 영상 모두에 facial landmark 정보가 있을 때만 거리 계산\n",
    "        if len(clips_frame_info[0][i]) > 0 and (clips_frame_info[1][i+1]) > 0:\n",
    "            # 양쪽 눈 끝만 사용해도 특징을 표현할 수 있음\n",
    "            l = 36\n",
    "            r = 45\n",
    "            left_eye = (\n",
    "                (clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][1]) ** 2 +\\\n",
    "                (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1]) ** 2\n",
    "            ) ** 0.5\n",
    "            right_eye = (\n",
    "                (clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][1]) ** 2 +\\\n",
    "                (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1]) ** 2\n",
    "            ) ** 0.5\n",
    "            # 각 frame별 두 얼굴 사이의 거리를 dist_arr에 붙여넣음\n",
    "            total_diff = left_eye + right_eye\n",
    "            dist_arr.append(total_diff)\n",
    "        else:\n",
    "            dist_arr.append(None)\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e14e8",
   "metadata": {},
   "source": [
    "### - 5. 얼굴 기반 영상의 거리를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddc700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(self, reference_clip, compare_clip):\n",
    "    # 1. 거리 계산에 필요한 정보 얻기\n",
    "    # extract_landmark 함수를 이용해 각 영상마다 frame들의 facial landmark 정보가 담긴 clips_frame_info 변수 구하기\n",
    "    clips_frame_info = self.extract_landmark(reference_clip, compare_clip)\n",
    "    # min 함수로 더 짧은 영상을 기준으로 size 지정\n",
    "    min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "    # get_all_frame_distance 함수를 통해 모든 frame의 거리 정보가 담긴 dist_arr을 얻음\n",
    "    dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "    # 최소거리와, 최소거리를 가지는 index를 저장하기 위해 초기화\n",
    "    clips = [reference_clip, compare_clip]\n",
    "    minimax_frames = self.minimax_frames\n",
    "    min_diff = np.float('Inf')\n",
    "    min_idx = 0\n",
    "    # 2. 최소 거리가 되는 영상과 시간 찾기\n",
    "    # self.minimax_frames 개수만큼의 frame 확인\n",
    "    for i in range(min_size - (minimax_frames - 1)):\n",
    "        # 해당 frame 앞뒤의 거리 정보를 다 비교해야 함\n",
    "        # 첫 번째 frame에서는 이전 frame이 없음\n",
    "        # 이전 frame이 없는 frame일 경우에는 시작점을 0으로 잡도록 함\n",
    "        # 그렇지 않은 경우에는 해당 frame index에서 minimax_frame값을 빼서 앞뒤 frame을 거리 계산에 고려\n",
    "        start_minmax_idx = 0 if (i - minimax_frames) < 0 else i - minimax_frames\n",
    "        # 해당 frame과 minimax_frames 만큼 앞뒤에 얼굴 자체가 없는(None) 경우가 있는지 확인\n",
    "        if(None not in dist_arr[start_minimax_idx :i + minimax_frames]):\n",
    "            # np.max 함수를 통해 해당 범위 내의 최댓값(tmp_max)을 얻음\n",
    "            # 해당 영상의 대표값이 되는 최댓값이 최소거리(min_diff)보다 작은지 판단\n",
    "            tmp_max = np.max(dist_arr[start_minmax_idx:i + minimax_frames])\n",
    "            if min_diff > tmp_max:\n",
    "                min_diff = tmp_max\n",
    "                min_dix = i\n",
    "                \n",
    "    # 3. face embedding penalty 추가하기\n",
    "    if self.face_embedding_penalty != None and min_diff < np.float('Inf'):\n",
    "        ref_frame = reference_clip.get_frame(min_idx * 1.0 / reference_clip.fps)\n",
    "        frame = compare_clip.get_frame(min_idx * 1.0 / compare_clip.fps)\n",
    "        cosine_dist = self.embedding_cosine_distance(ref_frame, frame)\n",
    "        min_diff += cosine_dist * self.face_embedding_penalty\n",
    "    # 4. 두 영상 간의 최소 거리 정보 return\n",
    "    return min_diff, (min_idx * self.skip_frame_rate) / self.clips[0].fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a16fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c051221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9a4371d",
   "metadata": {},
   "source": [
    "# 3. PoseDistance Class 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5e5a4",
   "metadata": {},
   "source": [
    "### - 1. 초기화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12141c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    # 두 영상 간의 거리를 계산할 frame 단위\n",
    "    self.SKIP_FRAME_RATE = 10\n",
    "    # 거리를 계산할 때 확인하는 앞뒤 frame 수\n",
    "    self.MINIMAX_FRAME = 4\n",
    "    # pytorch pose estimation - alphapose -> faster R-CNN모델 사용\n",
    "    self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "    self.model.eval()\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58778e80",
   "metadata": {},
   "source": [
    "### - 2. 가수의 위치를 파악하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73574879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxes(self, reference_clip, compare_clip):\n",
    "    # 1. 변수 초기화\n",
    "    self.clips =[reference_clip, compare_clip]\n",
    "    clips_frame_info = []\n",
    "    for clip in self.clips:\n",
    "        # 2. 각 영상의 정보를 저장하기 위해 loop마다 초기화하기\n",
    "        i = 0\n",
    "        every_frame_info = []\n",
    "        # 영상의 끝 부분까지 pose box를 얻기 위한 while문\n",
    "        while True :\n",
    "            # 3. Faster R-CNN을 이용해 물체 판별하기\n",
    "            # 다음 loop에서 사용할 frame index를 저장\n",
    "            i += self.SKIP_FRAME_RATE\n",
    "            # clip.fps : 1초당 들어있는 frame 수\n",
    "            # 1.0 / clip.fps : 한 frame이 차지하는 시간\n",
    "            # i * 1.0 / clip.fps : 현재 확인중인 frame의 시간대\n",
    "            # 현재 확인 중인 frame의 시간(i*1.0/clip.fps)이 영상의 전체 시간(clip.duration)보다 크면 break\n",
    "            if (i * 1.0 / clip.fps) > clip.duration:\n",
    "                break\n",
    "            \n",
    "            # get_frame : Moviepy 제공 함수. 해당 시간의 frame을 이미지로 바꿔서 frame에 저장\n",
    "            frame = clip.get_frame(i * 1.0 / clip.fps)\n",
    "            # frame 크기 조정 함수\n",
    "            frame = imutils.resize(frame, width = 640)\n",
    "            # 원래 get_frame으로 얻은 이미지는 0~255 사이 수를 가짐\n",
    "            # alphapose에 입력되는 이미지는 0~1 사이 수를 가짐\n",
    "            # 그러므로 모든 수를 255로 나눠줘야 함\n",
    "            frame = frame / 255\n",
    "            # get_frame으로 얻은 이미지는 height, width, channel 순서임\n",
    "            frame = np.transpose(frame, (2, 0, 1))\n",
    "            # numpy 배열을 pytorch에서 사용 가능하도록 바꿔줌\n",
    "            x = [torch.from_numpy(frame).float()]\n",
    "            # input x를 faster r-cnn 모델에 넣어서 결괏값을 받음\n",
    "            # GPU는 predictions가 여러개가 있을 수 있음\n",
    "            # CPU는 predictions가 list 안에서 하나의 결과만을 가짐\n",
    "            # 그러므로 첫번째 predictions를 prediction으로 저장해줌\n",
    "            predictions = self.model(x)\n",
    "            prediction = predictions[0]\n",
    "            \n",
    "            # 4. 판별정보 재가공\n",
    "            # 두개의 list에서 같은 index의 값을 묶어주는 zip 함수\n",
    "            each_box_list = zip(prediction['boxes'].tolist(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "            # filter : 첫번째 argument로 함수를 받고, 두번째 argument로 list를 받아서\n",
    "            #          함수의 조건에 맞는 list 요소만 반환함\n",
    "            filtered_box_list = filter(lambda x: x[1] == 1 and x[2] >= 0.95, each_box_list)\n",
    "            # map : 첫번째 argument로 함수를 받고, 두번째 argument로 list를 받아\n",
    "            #       각 list의 요소에 함수를 하나씩 반영\n",
    "            filtered_center_dot_list = list(map(lambda x: [(x[0][0] + x[0][2]) / 2, (x[0][1] + x[0][3]) / 2], filtered_box_list))\n",
    "            # list 정렬\n",
    "            sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x: x[0])\n",
    "            \n",
    "            # 5. 재가공한 정보 every_frame_info에 저장\n",
    "            every_frame_info.append(sorted_dot_list)\n",
    "            \n",
    "        # 6. 영상 frame별 landmark 정보 clips_frame_info에 저장\n",
    "        clips_frame_info.append(np.array(every_frame_info))\n",
    "    return clips_frame_info\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b0864",
   "metadata": {},
   "source": [
    "### - 3. 두 영상의 frame간 거리를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda96ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "    dist_arr = list()\n",
    "    for i in range(min_size):\n",
    "        # 두 영상 모드에서 pose 정보가 있을 ㄸ만 거리를 계산\n",
    "        if len(clips_frame_info[0][i]) > 0 and len(clips_frame_info[1][i]) > 0:\n",
    "            # 같은 시간을 기준으로 두 영상 frame의 pose boxes 정보를 얻음\n",
    "            ref_frame_dots = clips_frame_info[0][i]\n",
    "            compare_frame_dots = clips_frame_info[1][i]\n",
    "            # 둘 다 box가 있어 둘 사이의 거리를 계산할 수 있을 때만 거리를 더하도록 점의 최소 개수 구하기\n",
    "            min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots))\n",
    "            # 두 영상의 점의 개수 차이를 dot_num_diff에 저장\n",
    "            dot_num_diff = abs(len(ref_frame_dots) - len(compare_frame_dots))\n",
    "            # penalty : 영상 중 하나가 점이 없어서 거리를 계산할 수 없을 때 더하는 가장 먼 거리의 값\n",
    "            # 영상의 끝과 끝의 대각선이 가장 긴 거리\n",
    "            penalty = ((self.clips[0].w ** 2 + self.clips[0].h ** 2) ** 0.5) * abs(len(ref_frame_dots) - len(compare_frame_dots))\n",
    "            # 두 영상 중 하나에 점이 없으면 해당 부분에 penalty\n",
    "            total_diff = penalty * dot_num_diff\n",
    "            # 두 영상에서 가까운 x좌표끼리의 거리를 계산해서 total_diff에 추가\n",
    "            for dot_idx in range(min_dot_num):\n",
    "                total_diff += ((ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0]) ** 2 + (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][1]) ** 2) ** 0.5\n",
    "                # 두 영상 frame간의 거리를 dist_arr에 추가\n",
    "                dist_arr.append(total_diff)\n",
    "            else :\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f1e64",
   "metadata": {},
   "source": [
    "### - 4. 거리 측정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f74ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(self, reference_clip, compare_clip):\n",
    "    # 1. 거리 계산에 필요한 정보들 먼저 얻기\n",
    "    # extract_boxes 함수를 이용해 각 영상마다 frame들의 box정보가 담긴 clips_frame_info 변수를 구함\n",
    "    clips_frame_info = self.extract_boxes(reference_clip, compare_clip)\n",
    "    # min 함수로 더 짧은 영상을 기준으로 size를 지ㅇ\n",
    "    min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "    # 모든 frame의 거리 정보가 담긴 dist_arr을 얻음\n",
    "    dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "    # 최소 거리와, 최소 거리를 가지는 index를 저장하기 위해 초기화\n",
    "    min_diff = np.float('Inf')\n",
    "    min_idx = 0\n",
    "    \n",
    "    # 2. 최소 거리가 되는 영상과 시간 찾기\n",
    "    # 끝부분은 self.MINIMAX_FRAME만큼의 frame을 확인하지 못하므로 건너뜀\n",
    "    for i in range(min_size - (self.MINIMAX_FRAME - 1)):\n",
    "        # 첫번째 frame에서는 이전 frame이 없음\n",
    "        # 이전 frame이 없는 frame일 경우 시작점을 0으로 잡음\n",
    "        start_minmax_idx = 0 if (i - self.MINIMAX_FRAME) < 0 else i - self.MINIMAX_FRAME\n",
    "        # 해당 frame과 self.MINIMAX_FRAME만큼의 앞뒤에 얼굴 자체가 없는(None) 경우가 있는지 확인\n",
    "        if (None not in dist_arr[start_minmax_idx : i + self.MINIMAX_FRAME]):\n",
    "            # np.max 함수를 통해 해당 범위 내의 최댓값(tmp_max)을 얻음\n",
    "            # 최댓값이 최소 거리(min_diff)보다 작은지 판단\n",
    "            tmp_max = np.max(dist_arr[i \n",
    "                                     : i + self.MINIMAX_FRAME])\n",
    "            if min_diff > tmp_max:\n",
    "                min_diff = tmp_max\n",
    "                min_idx = i\n",
    "        \n",
    "    # 3. 두 영상 간의 최소 거리 정보 반환\n",
    "    return min_diff, (min_idx * self.SKIP_FRAME_RATE) / reference_clip.fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fb7b5",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28807",
   "metadata": {},
   "source": [
    "## Crosscut Class 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c9a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crosscut:\n",
    "    def __init__(self, dist_obj, video_path, output_path):\n",
    "        self.videos_path = video_path\n",
    "        self.output_path = output_path\n",
    "        self.min_time = 1000.0\n",
    "        video_num = len(os.listdir(self.videos_path))\n",
    "        self.start_times = [0] * video_num\n",
    "        self.window_time = 10\n",
    "        self.padded_time = 4\n",
    "        self.dist_obj = dist_obj\n",
    "        self.audioclip = None\n",
    "        self.extracted_clips_array = []\n",
    "        self.con_clips = []\n",
    "        \n",
    "    def video_alignment(self):\n",
    "        for i in range(len(os.listdir(self.videos_path))):\n",
    "            video_path = os.path.join(self.videos_path, sorted(os.listdir(self.videos_path))[i])\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "            if self.min_time > clip.duration:\n",
    "                self.audioclip = clip.audio\n",
    "                self.min_time = clip.duration\n",
    "            self.extracted_clips_array.append(clip)\n",
    "        print('Logger-- {} Video Will Be Mixed'.format(len(self.extracted_clips_array)))\n",
    "        \n",
    "    def select_next_clip(self, t, current_idx):\n",
    "        cur_t = t\n",
    "        next_t = min(t + self.window_time, self.min_time)\n",
    "        \n",
    "        reference_clip = self.extracted_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "        d = float('Inf')\n",
    "        cur_clip = None\n",
    "        min_idx = (current_idx + 1) % len(self.extracted_clips_array)\n",
    "        for video_idx in range(len(self.extracted_clips_array)):\n",
    "            if video_idx == current_idx:\n",
    "                continue\n",
    "            clip = self.extracted_clips_array[video_idx].subclip(cur_t, next_t)\n",
    "            cur_d, plus_frame = self.dist_obj.distance(reference_clip, clip)\n",
    "            print(current_idx, video_idx, cur_d, cur_t + plus_frame)\n",
    "            if d > cur_d:\n",
    "                d = cur_d\n",
    "                min_idx = video_idx\n",
    "                next_t = cur_t + plus_frame\n",
    "                cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "                \n",
    "        if cur_clip:\n",
    "            clip = cur_clip\n",
    "        else:\n",
    "            clip = reference_clip\n",
    "        self.con_clips.append(clip)\n",
    "        t = next_t\n",
    "        return t, min_idx\n",
    "    \n",
    "    def add_padding(self, t, next_idx):\n",
    "        print('idx : {}'.format(next_idx))\n",
    "        pad_clip = self.extracted_clips_array[next_idx].subclip(t, min(self.min_time, t+self.padded_time))\n",
    "        self.con_clips.append(pad_clip)\n",
    "        \n",
    "        t = min(self.min_time, t + self.padded_time)\n",
    "        return t, next_idx\n",
    "    \n",
    "    def write_video(self):\n",
    "        final_clip = concatenate_videoclips(self.con_clips)\n",
    "        if self.audioclip != None:\n",
    "            print('Not None')\n",
    "            final_clip.audio = self.audioclip\n",
    "        final_clip.write_videofile(self.output_path)\n",
    "        return final_clip\n",
    "    \n",
    "    def generate_video(self):\n",
    "        self.video_alignment()\n",
    "        t = 3\n",
    "        current_idx = 0\n",
    "        self.con_clips.append(self.extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time))))\n",
    "        while t < int(self.min_time):\n",
    "            t, min_idx = self.select_next_clip(t, current_idx)\n",
    "            t, current_idx = self.add_padding(t, min_idx)\n",
    "        final_clip = self.write_video()\n",
    "        return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7384bf",
   "metadata": {},
   "source": [
    "## face distance 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e21ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDistance:\n",
    "    def __init__(self, shape_predictor_path, face_embedding_penalty = None):\n",
    "        self.skip_frame_rate = 4\n",
    "        self.minimax_frames = 5\n",
    "        self.shape_predictor = shape_predictor_path\n",
    "        self.face_embedding_penalty = face_embedding_penalty\n",
    "        \n",
    "    def extract_landmark(self, reference_clip, compare_clip):\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(self.shape_predictor)\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            while True:\n",
    "                frame = clip.get_frame(i * 1.0 / clip.fps)\n",
    "                i += self.skip_frame_rate\n",
    "                if (i * 1.0 / clip.fps) > clip.duration:\n",
    "                    break\n",
    "                    \n",
    "                frame = imutils.resize(frame, width = 800)\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "                if len(rects) > 0:\n",
    "                    max_width = 0\n",
    "                    max_rect = None\n",
    "                    for rect in rects:\n",
    "                        if int(rects[0].width()) > max_width:\n",
    "                            max_rect = rect\n",
    "                    shape = predictor(gray, max_rect)\n",
    "                    shape = face_utils.shape_to_np(shape)\n",
    "                    every_frame_info.append(shape)\n",
    "                else : \n",
    "                    every_frame_info.append([])\n",
    "                    \n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "        cv2.destroyAllWindows()\n",
    "        return clips_frame_info\n",
    "    \n",
    "    def embedding_cosine_distance(self, reference_frame, compare_frame):\n",
    "        face_detector = MTCNN(select_largest = True)\n",
    "        embed_model = InceptionResnetV1(pretrained = 'vggface2').eval()\n",
    "        \n",
    "        reference_frame = np.array(reference_frame)\n",
    "        compare_frame = np.array(compare_frame)\n",
    "        try:\n",
    "            reference_frame_detected = face_detector(reference_frame)\n",
    "            compare_frame_detected = face_detector(compare_frame)\n",
    "        except:\n",
    "            cosine_dist = 1\n",
    "            return cosine_dist\n",
    "        \n",
    "        reference_frame_embed = embed_model(reference_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        compare_frame_embed = embed_model(compare_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        reference_frame_embed = np.squeeze(reference_frame_embed)\n",
    "        compare_frame_embed = np.squeeze(compare_frame_embed)\n",
    "        cosine_dist = 1 -\\\n",
    "                    np.dot(reference_frame_embed, compare_frame_embed) /\\\n",
    "                    (np.linalg.norm(reference_frame_embed) *\\\n",
    "                     np.linalg.norm(compare_frame_embed))\n",
    "        return cosine_dist\n",
    "    \n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = []\n",
    "        for i in range(min_size - 1):\n",
    "            if len(clips_frame_info[0][i]) > 0 and len(clips_frame_info[1][i+1]):\n",
    "                l = 36\n",
    "                r = 45\n",
    "                left_eye = (\n",
    "                            (clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][0]) ** 2 +\\\n",
    "                            (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1]) ** 2\n",
    "                           ) ** 0.5\n",
    "                \n",
    "                right_eye = (\n",
    "                             (clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][0]) ** 2+\\\n",
    "                             (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1]) ** 2   \n",
    "                            ) ** 0.5\n",
    "                total_diff = left_eye + right_eye\n",
    "                dist_arr.append(total_diff)\n",
    "            else :\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "    \n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        clips_frame_info = self.extract_landmark(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        clips = [reference_clip, compare_clip]\n",
    "        minimax_frames = self.minimax_frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        for i in range(min_size - (minimax_frames - 1)):\n",
    "            start_minmax_idx = 0 if (i - minimax_frames) < 0 else i - minimax_frames\n",
    "            if (None not in dist_arr[start_minmax_idx :i + minimax_frames]):\n",
    "                tmp_max = np.max(dist_arr[start_minmax_idx:i + minimax_frames])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "                    \n",
    "        if self.face_embedding_penalty != None and min_diff < np.float('Inf'):\n",
    "            ref_frame = reference_clip.get_frame(min_idx * 1.0 / reference_clip.fps)\n",
    "            frame = compare_clip.get_frame(min_idx * 1.0 / compare_clip.fps)\n",
    "            cosine_dist = self.embedding_cosine_distance(ref_frame, frame)\n",
    "            min_diff += cosine_dist * self.face_embedding_penalty\n",
    "            \n",
    "        return min_diff, (min_idx * self.skip_frame_rate) / self.clips[0].fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08489d",
   "metadata": {},
   "source": [
    "## Pose distance 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4748e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistance:\n",
    "    def __init__(self):\n",
    "        self.SKIP_FRAME_RATE = 10\n",
    "        self.MINIMAX_FRAME = 4\n",
    "        self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "        self.model.eval()\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "        \n",
    "    def extract_boxes(self,reference_clip, compare_clip):\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            while True:\n",
    "                i += self.SKIP_FRAME_RATE\n",
    "                if (i * 1.0 / clip.fps) > clip.duration:\n",
    "                    break\n",
    "                frame = clip.get_frame(i * 1.0 / clip.fps)\n",
    "                frame = imutils.resize(frame, width = 640)\n",
    "                frame = frame / 255\n",
    "                frame = np.transpose(frame, (2, 0, 1))\n",
    "                x = [torch.from_numpy(frame).float()]\n",
    "                predictions = self.model(x)\n",
    "                prediction = predictions[0]\n",
    "                each_box_list = zip(prediction['boxes'].tolist(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "                filtered_box_list = filter(lambda x: x[1] == 1 and x[2] >= 0.95, each_box_list)\n",
    "                filtered_center_dot_list = list(map(lambda x: [(x[0][0] + x[0][2]) / 2, (x[0][1] + x[0][3]) / 2], filtered_box_list))\n",
    "                sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x: x[0])\n",
    "                every_frame_info.append(sorted_dot_list)\n",
    "                \n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "        return clips_frame_info\n",
    "    \n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = list()\n",
    "        for i in range(min_size):\n",
    "            if len(clips_frame_info[0][i]) > 0 and len(clips_frame_info[1][i]) > 0:\n",
    "                ref_frame_dots = clips_frame_info[0][i]\n",
    "                compare_frame_dots = clips_frame_info[1][i]\n",
    "                min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots))\n",
    "                dot_num_diff = abs(len(ref_frame_dots) - len(compare_frame_dots))\n",
    "                penalty = ((self.clips[0].w ** 2 + self.clips[0].h ** 2) ** 0.5) * abs(len(ref_frame_dots) - len(compare_frame_dots))\n",
    "                total_diff = penalty * dot_num_diff\n",
    "                for dot_idx in range(min_dot_num):\n",
    "                    total_diff += (\n",
    "                        (ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0]) ** 2 +\\\n",
    "                        (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][0]) ** 2\n",
    "                    ) ** 0.5\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "    \n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        clips_frame_info = self.extract_boxes(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        for i in range(min_size - (self.MINIMAX_FRAME - 1)):\n",
    "            start_minmax_idx = 0 if (i - self.MINIMAX_FRAME) < 0 else i - self.MINIMAX_FRAME\n",
    "            if (None not in dist_arr[start_minmax_idx :i + self.MINIMAX_FRAME]):\n",
    "                tmp_max = np.max(dist_arr[i:i + self.MINIMAX_FRAME])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "        return min_diff, (min_idx * self.SKIP_FRAME_RATE) / reference_clip.fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488b580",
   "metadata": {},
   "source": [
    "# 4. 교차편집 실행 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71469fd4",
   "metadata": {},
   "source": [
    "### - 1. library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from moviepy.editor import *\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0efe0",
   "metadata": {},
   "source": [
    "### - 2. 교차편집 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf44517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./yys_pose.mp4\n",
      "Logger-- 5 Video Will Be Mixed\n",
      "0 1 202.20843159761407 8.672333333333334\n",
      "0 2 inf 3.0\n",
      "0 3 inf 3.0\n",
      "0 4 inf 3.0\n",
      "idx : 1\n",
      "1 0 166.92133982568072 19.012\n",
      "1 2 163.9601369913043 17.343666666666667\n",
      "1 3 inf 12.672333333333334\n",
      "1 4 inf 12.672333333333334\n",
      "idx : 2\n",
      "2 0 93.00789344259424 22.678333333333335\n"
     ]
    }
   ],
   "source": [
    "# random, face, pose중에 하나\n",
    "method = 'pose'\n",
    "# 무대 영상들이 저장된 폴더\n",
    "video_path = './ex_video'\n",
    "# 교차편집 영상을 저장할 경로와 파일명\n",
    "output_path = './yys_pose.mp4'\n",
    "# 얼굴의 landmark를 추출할 때 사용하는 predictor의 경로\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "# 얼굴의 embedding 값으로 penalty를 줄지. 값이 클 수록 강한 penalty\n",
    "face_embedding_penalty = 100 # or None\n",
    "# 선택한 method에 따라 거리 측정 객체를 생성하고 해당 객체를 전달 해 교차편집 객체 cross_cut 생성\n",
    "# 교차편집 생성함수 generate_video()를 이용해 교차편집을 만들어 저장\n",
    "print(output_path)\n",
    "if method == 'random':\n",
    "    random_distance = RandomDistance()\n",
    "    cross_cut = Crosscut(random_distance, video_path, output_path)\n",
    "elif method == 'face':\n",
    "    face_distance = FaceDistance(shape_predictor_path, face_embedding_penalty)\n",
    "    cross_cut = Crosscut(face_distance, video_path, output_path)\n",
    "elif method == 'pose':\n",
    "    pose_distance = PoseDistance()\n",
    "    cross_cut = Crosscut(pose_distance, video_path, output_path)\n",
    "\n",
    "cross_cut.generate_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d1a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96515b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa927a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac3006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94f9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4928e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97c05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4010b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f5b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b2e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003bb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314c0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09259f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820048dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af5f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d57cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d207a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb452f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeed58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcbb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657fd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a7c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e44bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6697fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0091e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abec93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a7c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b52ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb2711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3be812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acecc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fb990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125917f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00b43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4578a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96088b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f82f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f223527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5e024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dc0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337b22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eef11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b9974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5108c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd894d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57914e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8feaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191b573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da790fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4912f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf9727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678980e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1ed70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a90d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34294c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d14c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603304e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8099b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16cd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbb6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20730d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f0d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4af7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2286d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c2c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102df7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7f98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef8f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505106b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7701455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f387a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8043ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e13f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe9bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee8858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd8cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb645f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922d3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f3563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883be876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0878c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf17e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d950a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aed641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b56da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e3c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cf38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef0428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
