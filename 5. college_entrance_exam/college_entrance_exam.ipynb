{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb51bc4",
   "metadata": {},
   "source": [
    "# 1. LSTM 기본 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14c23",
   "metadata": {},
   "source": [
    "### - 1. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7712cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rkoh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# legacy not found error -> pip install torchtext==0.10.0 이후 재부팅\n",
    "\n",
    "import dill\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "from torchtext.legacy.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088001a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2020\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = 'data/processed/'\n",
    "#DATA_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa98bb3",
   "metadata": {},
   "source": [
    "### - 2. 모델 클래스 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4a13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(\n",
    "            # 생성할 Embedding Layer의 크기 정해주기\n",
    "            # 보통 단어장 크기\n",
    "            num_embeddings = num_embeddings,\n",
    "            embedding_dim = embedding_dim,\n",
    "            # 자연어 처리에서 배치별로 문장의 크기를 맞추기 위해서 짧은 문장에 Padding을 붙여서 길이를 맞춤\n",
    "            # 특별한 의미는 없음\n",
    "            # 학습에서 제외하기 위해 Padding이 단어장에서 어떤 숫자를 갖고 있는지 알려줌으로써 학습되지 않게 함\n",
    "            padding_idx = pad_idx\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            bidirectional = True,\n",
    "            dropout = 0.5\n",
    "        )\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(),\n",
    "            # 가장 마지막 output 크기를 1로 줌\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            # 확률로 변환시키기 위해 Sigmoid를 마지막 Activation으로 줌\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabf0f8",
   "metadata": {},
   "source": [
    "### - 3. 모델 파이프라인 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff3ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    # 숫자로 이루어진 토큰을 Input으로 받는다고 가정\n",
    "    # Input값을 Embedding 값으로 변환시켜주어야 함\n",
    "    embed_x = self.embed_layer(x)\n",
    "    # LSTM은 output, (Hidden State, Cell State)를 반환함\n",
    "    # 이 중 State 값들은 사용하지 않으므로 반환받지 않음\n",
    "    output, (_, _) = self.lstm_layer(embed_x)\n",
    "    # LSTM의 output은 (배치 크기, 문장 길이, output size) size를 가짐\n",
    "    # 가장 마지막 단어의 결괏값을 사용\n",
    "    last_output = output[:, -1, :]\n",
    "    # 문장의 마지막 단어의 output을 Fully Connected Layer에 통과시켜 확률값 계산\n",
    "    last_output = self.last_layer(last_output)\n",
    "    return last_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf14663",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea5b79",
   "metadata": {},
   "source": [
    "### - 0. torchtext 사용 순서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6407379e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1841777516.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    TEXT = Field(..)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 파일에서 필요한 필드 선언\n",
    "TEXT = Field(..)\n",
    "LABEL = Field(..)\n",
    "# 데이터 불러오기\n",
    "dataset = TabularDataset(..)\n",
    "# 불러온 데이터 단어장 만들기\n",
    "TEXT.build_vocab(dataset)\n",
    "# Data loader 만들기\n",
    "data_loader = BucketIterator(dataset, ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c8700",
   "metadata": {},
   "source": [
    "### - 1. 문장 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b111ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(\n",
    "    # 이 필드에는 문장이 들어온다는 것을 알려주는 True\n",
    "    sequential = True,\n",
    "    # 단어를 숫자로 변환시켜주는 단어장을 만들기 위해 이 필드 사용\n",
    "    use_vocab = True,\n",
    "    # 불러올 문장을 토크나이징할 함수 입력\n",
    "    # word tokenize는 영어로 이루어진 문장을 토큰화시킬 때 가장 기본적으로 사용\n",
    "    tokenize = word_tokenize,\n",
    "    # 대소문자 구분.\n",
    "    # lower = True : 모두 소문자 처리\n",
    "    lower = True,\n",
    "    # 자연어 처리 모듈별로 지원하는 데이터 형태\n",
    "    # True : (배치, 문장)\n",
    "    # False : (문장, 배치)\n",
    "    batch_first = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae17202",
   "metadata": {},
   "source": [
    "### - 2. 정답 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "692c1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = Field(\n",
    "    # 문장 필드는 문장이 들어오기 때문에 True, 해당 열은 정답이 있으므로 False\n",
    "    sequential = False,\n",
    "    # 단어장을 생성하지 않음\n",
    "    use_vocab = False,\n",
    "    batch_first = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97cbbad",
   "metadata": {},
   "source": [
    "### - 3. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e6323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_train_data, sat_valid_data, sat_test_data = \\\n",
    "    TabularDataset.splits(\n",
    "        # 데이터가 들어있는 폴더 경로   \n",
    "        path = 'data/processed/',\n",
    "        # 각각 train, val, test 파일명\n",
    "        train = 'sat_train.tsv',\n",
    "        validation = 'sat_valid.tsv',\n",
    "        test = 'sat_test.tsv',\n",
    "        # 데이터의 파일 포맷 형태. Tap Separated Value\n",
    "        format = 'tsv',\n",
    "        # 정의한 Field 입력\n",
    "        # 실제 데이터 컬럼 순서로 입력해주어야 함\n",
    "        # ('text', TEXT) : 이 데이터는 첫 번째 컬럼에 문장이 있고 그 컬럼명을 text로 하겠다\n",
    "        fields = [('text', TEXT), ('label', LABEL)],\n",
    "        # 데이터의 첫 번째 열에는 원래의 컬럼명이 들어 있음\n",
    "        # 데이터로 사용되지 않기 때문에 따로 불러오지 않도록 해야 함\n",
    "        # 1을 주어서 첫 번째 열 생략\n",
    "        skip_header = 1        \n",
    "    )\n",
    "# 마지막으로 불러온 데이터 중 훈련 데이터를 이용해 TEXT 단어장 생성\n",
    "# 그 중 2번 이상 나온 단어만 단어장에 사용\n",
    "TEXT.build_vocab(sat_train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bd138",
   "metadata": {},
   "source": [
    "### - 4. data loader 정의\n",
    "- 문장의 길이를 보고 길이가 비슷한 문장끼리 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f20093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_train_iterator, sat_valid_iterator, sat_test_iterator = \\\n",
    "    BucketIterator.splits(\n",
    "        # 앞에서 불러온 데이터들을 묶어서 입력\n",
    "        (sat_train_data, sat_valid_data, sat_test_data),\n",
    "        # Data Loader에서 각 배치별 크기\n",
    "        batch_size = 8,\n",
    "        device = None,\n",
    "        sort = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad6991",
   "metadata": {},
   "source": [
    "# 3. 학습\n",
    "1. Data Loader에서 배치 불러오기\n",
    "2. 배치를 모델에 넣어서 데이터 형태 맞추기\n",
    "3. 배치를 모델에 넣어서 예측값 얻기\n",
    "4. 정답과 예측값을 비교해서 Loss 계산하기\n",
    "5. Loss를 이용해 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a622f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_x = self.embed_layer(x)\n",
    "        output, (_, _) = self.lstm_layer(embed_x)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.last_layer(last_output)\n",
    "        return last_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5895187",
   "metadata": {},
   "source": [
    "### - 1. 모델 학습 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436ec43",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137ede07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # 입력 받은 Data Loader를 호출해 Batch를 부르는 코드\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Batch는 두개의 Attribute를 갖고 있음 - fields = [('text', TEXT), ('label', LABEL)]\n",
    "        # 'text'는 batch의 문장, 'label'은 Batch의 정답\n",
    "        text = batch.text\n",
    "        if text.shape[0] > 1:\n",
    "            # 문장과 정답을 불러와서 필요한 데이터 형태로 변환\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            # 모델에 문장을 넣어서 결과 출력\n",
    "            output = model(text).flatten()\n",
    "            # 출력된 결과와 정답을 비교해서 Loss를 구함\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85e73",
   "metadata": {},
   "source": [
    "### - 2. 모델 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e8ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader, criterion, device):\n",
    "    # 평가를 위한 eval()\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    # torch에서는 기본적으로 forward를 할 때 자동으로 gradient를 계산함\n",
    "    # 평가할 때는 gradient 계산이 필요가 없음\n",
    "    # 그래서 torch.no_grad()를 통해 gradient가 계산되지 않도록 함\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(valid_loader):\n",
    "            text = batch.text\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(text).flatten()\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f079e8f",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bab087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, iterator: Iterator, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: str):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text = batch.text\n",
    "        if text.shape[0] > 1:\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(text).flatten()\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, iterator: Iterator, criterion: nn.Module, device: str):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            text = batch.text\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(text).flatten()\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def test(model: nn.Module, iterator: Iterator, device: str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_real = []\n",
    "        y_pred = []\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            output = model(text).flatten().cpu()\n",
    "            y_real += [label]\n",
    "            y_pred += [output]\n",
    "        y_real = torch.cat(y_real)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    auroc = auc(fpr, tpr)\n",
    "\n",
    "    return auroc\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e3400",
   "metadata": {},
   "source": [
    "### - 3. HyperParameter 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4bde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer에 사용할 Padding Index를 가져옴\n",
    "# TEXT.vocab.stoi : 앞서 만든 단어장에서 단어를 토큰으로 만들어주는 Dictionary\n",
    "# TEXT.vocab.itos : 토큰을 단어로 바꿀 수 있음\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "N_EPOCHS = 20\n",
    "\n",
    "# 학습시킬 모델 정의\n",
    "lstm_classifier = LSTMClassifier(\n",
    "    num_embeddings = len(TEXT.vocab),\n",
    "    embedding_dim = 100,\n",
    "    hidden_size = 200,\n",
    "    num_layers = 4,\n",
    "    pad_idx = PAD_IDX\n",
    ")\n",
    "\n",
    "# CPU - GPU 정의\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "_ = lstm_classifier.to(device)\n",
    "\n",
    "# 가장 대중적으로 쓰이는 Optimizer - Adam\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters())\n",
    "# 손실 함수 : Binary Cross Entropy 사용\n",
    "bce_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb32db",
   "metadata": {},
   "source": [
    "### - 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ff57301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\n",
      "\tTrain Loss :  0.52055\n",
      "\t Val. Loss :  0.76535\n",
      "Epoch : 02\n",
      "\tTrain Loss :  0.51072\n",
      "\t Val. Loss :  0.55905\n",
      "Epoch : 03\n",
      "\tTrain Loss :  0.47557\n",
      "\t Val. Loss :  0.54456\n",
      "Epoch : 04\n",
      "\tTrain Loss :  0.43689\n",
      "\t Val. Loss :  0.53841\n",
      "Epoch : 05\n",
      "\tTrain Loss :  0.42774\n",
      "\t Val. Loss :  0.53824\n",
      "Epoch : 06\n",
      "\tTrain Loss :  0.40097\n",
      "\t Val. Loss :  0.54642\n",
      "Epoch : 07\n",
      "\tTrain Loss :  0.44003\n",
      "\t Val. Loss :  0.53912\n",
      "Epoch : 08\n",
      "\tTrain Loss :  0.42337\n",
      "\t Val. Loss :  0.53085\n",
      "Epoch : 09\n",
      "\tTrain Loss :  0.42087\n",
      "\t Val. Loss :  0.53794\n",
      "Epoch : 10\n",
      "\tTrain Loss :  0.43359\n",
      "\t Val. Loss :  0.53596\n",
      "Epoch : 11\n",
      "\tTrain Loss :  0.43982\n",
      "\t Val. Loss :  0.53834\n",
      "Epoch : 12\n",
      "\tTrain Loss :  0.46754\n",
      "\t Val. Loss :  0.53222\n",
      "Epoch : 13\n",
      "\tTrain Loss :  0.42592\n",
      "\t Val. Loss :  0.54775\n",
      "Epoch : 14\n",
      "\tTrain Loss :  0.42254\n",
      "\t Val. Loss :  0.58045\n",
      "Epoch : 15\n",
      "\tTrain Loss :  0.41939\n",
      "\t Val. Loss :  0.57019\n",
      "Epoch : 16\n",
      "\tTrain Loss :  0.42119\n",
      "\t Val. Loss :  0.55278\n",
      "Epoch : 17\n",
      "\tTrain Loss :  0.43622\n",
      "\t Val. Loss :  0.55147\n",
      "Epoch : 18\n",
      "\tTrain Loss :  0.43983\n",
      "\t Val. Loss :  0.56098\n",
      "Epoch : 19\n",
      "\tTrain Loss :  0.43902\n",
      "\t Val. Loss :  0.54761\n",
      "Epoch : 20\n",
      "\tTrain Loss :  0.42473\n",
      "\t Val. Loss :  0.56533\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(\n",
    "        lstm_classifier,\n",
    "        # 학습\n",
    "        sat_train_iterator,\n",
    "        optimizer,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    valid_loss = evaluate(\n",
    "        lstm_classifier,\n",
    "        # loss 계산\n",
    "        sat_valid_iterator,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'Epoch : {epoch + 1:02}')\n",
    "    print(f'\\tTrain Loss : {train_loss : .5f}')\n",
    "    print(f'\\t Val. Loss : {valid_loss : .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d8150",
   "metadata": {},
   "source": [
    "### - 5. 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c1e7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_model.dill', 'wb') as f:\n",
    "    model = {\n",
    "        'TEXT' : TEXT,\n",
    "        'LABEL' : LABEL,\n",
    "        'classifier' : lstm_classifier\n",
    "    }\n",
    "    dill.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3d822",
   "metadata": {},
   "source": [
    "# 4. Test\n",
    "- Area Under Receiver OperationCharacteristic(AUROC) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf8b78",
   "metadata": {},
   "source": [
    "### - 1. 테스트 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb25526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_real = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            text = batch.text\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            text = text.to(device)\n",
    "            \n",
    "            output = model(text).flatten().cpu()\n",
    "            \n",
    "            # Test 결과를 보기 위해 각 Batch의 예측값을 list에 모음\n",
    "            y_real += [label]\n",
    "            y_pred += [output]\n",
    "            \n",
    "        # 모인 예측값들을 합침\n",
    "        y_real = torch.cat(y_real)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "    # 예측값과 정답을이용해 AUROC 계산\n",
    "    fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    \n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b36a",
   "metadata": {},
   "source": [
    "### - 2. 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d3c2f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAT Dataset Test AUROC : 0.84615\n"
     ]
    }
   ],
   "source": [
    "_ = lstm_classifier.cpu()\n",
    "test_auroc = test(\n",
    "    # 학습이 끝난 모델 입력\n",
    "    lstm_classifier,\n",
    "    # 수능 test 데이터를 입력해 결과 확인\n",
    "    sat_test_iterator,\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "# 일반적으로 AUROC의 기준값은 0.5\n",
    "print(f'SAT Dataset Test AUROC : {test_auroc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a89c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926f2b52",
   "metadata": {},
   "source": [
    "# 5. 성능 높이기 - 추가 데이터 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de86023",
   "metadata": {},
   "source": [
    "### - 1. 사전 학습 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c76ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(\n",
    "    sequential = True,\n",
    "    use_vocab = True,\n",
    "    tokenize = word_tokenize,\n",
    "    lower = True,\n",
    "    batch_first = True\n",
    ")\n",
    "Label = Field(\n",
    "    sequential = False,\n",
    "    use_vocab = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "cola_train_data, cola_valid_data,cola_test_data = \\\n",
    "    TabularDataset.splits(\n",
    "        path = DATA_PATH,\n",
    "        train = 'cola_train.tsv',\n",
    "        validation = 'cola_valid.tsv',\n",
    "        test = 'cola_test.tsv',\n",
    "        format = 'tsv',\n",
    "        fields = [('text', TEXT), ('label', LABEL)],\n",
    "        skip_header = 1\n",
    "    )\n",
    "\n",
    "# 사전학습은 사전학습 때 이용한 모델의 단어장을 유지하는 것이 중요함\n",
    "# A모델과 B 모델에서의 단어는 같지만 토큰이 다르면 모델의 성능이 보장되지 않음\n",
    "TEXT.build_vocab(cola_train_data, min_freq = 2)\n",
    "cola_train_iterator, cola_valid_iterator, cola_test_iterator = \\\n",
    "    BucketIterator.splits(\n",
    "        (cola_train_data, cola_valid_data, cola_test_data),\n",
    "        batch_size = 32,\n",
    "        device = None,\n",
    "        sort = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9174f",
   "metadata": {},
   "source": [
    "### - 2. 추가 학습 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47f7cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_train_data, sat_valid_data,sat_test_data = \\\n",
    "    TabularDataset.splits(\n",
    "        path = DATA_PATH,\n",
    "    train = \"sat_train.tsv\",\n",
    "    validation = \"sat_valid.tsv\",\n",
    "    test = \"sat_test.tsv\",\n",
    "    format = \"tsv\",\n",
    "    # CoLA데이터에서 만든 Field\n",
    "    fields = [(\"text\", TEXT), (\"label\", LABEL)],\n",
    "    skip_header = 1\n",
    "    )\n",
    "\n",
    "sat_train_iterator, sat_valid_iterator,sat_test_iterator = \\\n",
    "    BucketIterator.splits(\n",
    "        (sat_train_data, sat_valid_data, sat_test_data),\n",
    "        batch_size = 8,\n",
    "        device = None,\n",
    "        sort = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85850b",
   "metadata": {},
   "source": [
    "### - 3. 모델 사전 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e5caeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\n",
      "\tTrain Loss : 0.61572\n",
      "\t Val. Loss : 0.61718\n",
      "Epoch : 02\n",
      "\tTrain Loss : 0.61236\n",
      "\t Val. Loss : 0.61897\n",
      "Epoch : 03\n",
      "\tTrain Loss : 0.61123\n",
      "\t Val. Loss : 0.61734\n",
      "Epoch : 04\n",
      "\tTrain Loss : 0.61139\n",
      "\t Val. Loss : 0.61739\n",
      "Epoch : 05\n",
      "\tTrain Loss : 0.61087\n",
      "\t Val. Loss : 0.61745\n",
      "Epoch : 06\n",
      "\tTrain Loss : 0.60869\n",
      "\t Val. Loss : 0.61889\n",
      "Epoch : 07\n",
      "\tTrain Loss : 0.60907\n",
      "\t Val. Loss : 0.61785\n",
      "Epoch : 08\n",
      "\tTrain Loss : 0.60964\n",
      "\t Val. Loss : 0.61831\n",
      "Epoch : 09\n",
      "\tTrain Loss : 0.60958\n",
      "\t Val. Loss : 0.62143\n",
      "Epoch : 10\n",
      "\tTrain Loss : 0.60851\n",
      "\t Val. Loss : 0.61852\n",
      "Epoch : 11\n",
      "\tTrain Loss : 0.60938\n",
      "\t Val. Loss : 0.61844\n",
      "Epoch : 12\n",
      "\tTrain Loss : 0.60842\n",
      "\t Val. Loss : 0.62926\n",
      "Epoch : 13\n",
      "\tTrain Loss : 0.60918\n",
      "\t Val. Loss : 0.61917\n",
      "Epoch : 14\n",
      "\tTrain Loss : 0.60811\n",
      "\t Val. Loss : 0.61830\n",
      "Epoch : 15\n",
      "\tTrain Loss : 0.60980\n",
      "\t Val. Loss : 0.61777\n",
      "Epoch : 16\n",
      "\tTrain Loss : 0.60883\n",
      "\t Val. Loss : 0.61978\n",
      "Epoch : 17\n",
      "\tTrain Loss : 0.60843\n",
      "\t Val. Loss : 0.61891\n",
      "Epoch : 18\n",
      "\tTrain Loss : 0.60803\n",
      "\t Val. Loss : 0.61875\n",
      "Epoch : 19\n",
      "\tTrain Loss : 0.60770\n",
      "\t Val. Loss : 0.61873\n",
      "Epoch : 20\n",
      "\tTrain Loss : 0.60819\n",
      "\t Val. Loss : 0.61752\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "N_EPOCHS = 20\n",
    "\n",
    "lstm_classifier = LSTMClassifier(\n",
    "    num_embeddings = len(TEXT.vocab),\n",
    "    embedding_dim = 100,\n",
    "    hidden_size = 200,\n",
    "    num_layers = 4,\n",
    "    pad_idx = PAD_IDX\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "_ = lstm_classifier.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters())\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(\n",
    "        lstm_classifier,\n",
    "        # CoLA 데이터를이용해 모델 학습\n",
    "        cola_train_iterator,\n",
    "        optimizer,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    valid_loss = evaluate(\n",
    "        lstm_classifier,\n",
    "        cola_valid_iterator,\n",
    "        bce_loss_fn,\n",
    "        device    \n",
    "    )\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss : {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss : {valid_loss:.5f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c43ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# 사전 학습한 모델과 추가 학습한 모델의 성능을 비교하기 위해 사전 학습한 모델 따로 저장\n",
    "before_tuning_lstm_classifier = deepcopy(lstm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1508b0",
   "metadata": {},
   "source": [
    "### - 4. 모델 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6d9998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\n",
      "\tTrain Loss : 0.48059\n",
      "\t Val. Loss : 0.54516\n",
      "Epoch : 02\n",
      "\tTrain Loss : 0.44044\n",
      "\t Val. Loss : 0.52474\n",
      "Epoch : 03\n",
      "\tTrain Loss : 0.43046\n",
      "\t Val. Loss : 0.50973\n",
      "Epoch : 04\n",
      "\tTrain Loss : 0.42282\n",
      "\t Val. Loss : 0.54304\n",
      "Epoch : 05\n",
      "\tTrain Loss : 0.43730\n",
      "\t Val. Loss : 0.55573\n",
      "Epoch : 06\n",
      "\tTrain Loss : 0.41017\n",
      "\t Val. Loss : 0.55850\n",
      "Epoch : 07\n",
      "\tTrain Loss : 0.42199\n",
      "\t Val. Loss : 0.56403\n",
      "Epoch : 08\n",
      "\tTrain Loss : 0.41851\n",
      "\t Val. Loss : 0.53660\n",
      "Epoch : 09\n",
      "\tTrain Loss : 0.39470\n",
      "\t Val. Loss : 0.53152\n",
      "Epoch : 10\n",
      "\tTrain Loss : 0.41769\n",
      "\t Val. Loss : 0.52412\n",
      "Epoch : 11\n",
      "\tTrain Loss : 0.43400\n",
      "\t Val. Loss : 0.52917\n",
      "Epoch : 12\n",
      "\tTrain Loss : 0.43127\n",
      "\t Val. Loss : 0.52669\n",
      "Epoch : 13\n",
      "\tTrain Loss : 0.42435\n",
      "\t Val. Loss : 0.52834\n",
      "Epoch : 14\n",
      "\tTrain Loss : 0.43298\n",
      "\t Val. Loss : 0.53807\n",
      "Epoch : 15\n",
      "\tTrain Loss : 0.41829\n",
      "\t Val. Loss : 0.53283\n",
      "Epoch : 16\n",
      "\tTrain Loss : 0.41945\n",
      "\t Val. Loss : 0.52609\n",
      "Epoch : 17\n",
      "\tTrain Loss : 0.42199\n",
      "\t Val. Loss : 0.52492\n",
      "Epoch : 18\n",
      "\tTrain Loss : 0.42326\n",
      "\t Val. Loss : 0.52838\n",
      "Epoch : 19\n",
      "\tTrain Loss : 0.39548\n",
      "\t Val. Loss : 0.52388\n",
      "Epoch : 20\n",
      "\tTrain Loss : 0.41450\n",
      "\t Val. Loss : 0.52241\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "N_EPOCHS = 20\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(\n",
    "        lstm_classifier,\n",
    "        sat_train_iterator,\n",
    "        optimizer,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    valid_loss = evaluate(\n",
    "        lstm_classifier,\n",
    "        sat_valid_iterator,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss : {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7218484",
   "metadata": {},
   "source": [
    "### - 5. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3960e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning SAT Dataset TEst AUROC : 0.76923\n",
      "After fine-tuning SAT Dataset TEst AUROC : 0.65385\n"
     ]
    }
   ],
   "source": [
    "_ = before_tuning_lstm_classifier.cpu()\n",
    "lstm_sat_test_auroc = test(\n",
    "    before_tuning_lstm_classifier, sat_test_iterator, \"cpu\"\n",
    ")\n",
    "\n",
    "_ = lstm_classifier.cpu()\n",
    "lstm_tuned_test_auroc = test(\n",
    "    lstm_classifier, sat_test_iterator, \"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Before fine-tuning SAT Dataset TEst AUROC : {lstm_sat_test_auroc:.5f}')\n",
    "print(f'After fine-tuning SAT Dataset TEst AUROC : {lstm_tuned_test_auroc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce784ac",
   "metadata": {},
   "source": [
    "### - 6. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac0f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('before_tuning_model.dill', 'wb') as f:\n",
    "    model = {\n",
    "        'TEXT' : TEXT,\n",
    "        'LABEL' : LABEL,\n",
    "        'classifier' : before_tuning_lstm_classifier\n",
    "    }\n",
    "    dill.dump(model,f)\n",
    "    \n",
    "_ = lstm_classifier.cpu()\n",
    "with open('after_tuning_model.dill', 'wb') as f:\n",
    "    model = {\n",
    "        'TEXT' : TEXT,\n",
    "        'LABEL' : LABEL,\n",
    "        'classifier' : lstm_classifier\n",
    "    }\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963a473",
   "metadata": {},
   "source": [
    "# 6. 심화 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddeb43",
   "metadata": {},
   "source": [
    "### - 1. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d979941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPoolingClassifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(\n",
    "            num_embeddings = num_embeddings,\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx = pad_idx\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            bidirectional = True,\n",
    "            dropout = 0.5,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 1),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Token 으로 들어온 데이터를 Emedding하여 값으로 변환\n",
    "        x = self.embed_layer(x)\n",
    "        # 변환된 값을 LSTM에 넣음\n",
    "        output, _ = self.lstm_layer(x)\n",
    "        # LSTM의 결과를 Max Pooling\n",
    "        pool = nn.functional.max_pool1d(output.transpose(1, 2), x.shape[1])\n",
    "        # Max Pooling 결과를 Fully Connected Layer에 넣기 위해 Shape를 맞춤\n",
    "        pool = pool.transpose(1, 2).squeeze()\n",
    "        # Fully Connected Layer에 넣어서 결과반환\n",
    "        output = self.last_layer(pool)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12742238",
   "metadata": {},
   "source": [
    "### - 2. 모델 사전 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3addd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkoh/anaconda3/envs/vip/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\n",
      "\tTrain Loss : 0.65178\n",
      "\t Val. Loss : 0.62461\n",
      "Epoch : 02\n",
      "\tTrain Loss : 0.65243\n",
      "\t Val. Loss : 0.62057\n",
      "Epoch : 03\n",
      "\tTrain Loss : 0.64198\n",
      "\t Val. Loss : 0.63551\n",
      "Epoch : 04\n",
      "\tTrain Loss : 0.64251\n",
      "\t Val. Loss : 0.62867\n",
      "Epoch : 05\n",
      "\tTrain Loss : 0.63104\n",
      "\t Val. Loss : 0.61683\n",
      "Epoch : 06\n",
      "\tTrain Loss : 0.62215\n",
      "\t Val. Loss : 0.62509\n",
      "Epoch : 07\n",
      "\tTrain Loss : 0.60219\n",
      "\t Val. Loss : 0.61948\n",
      "Epoch : 08\n",
      "\tTrain Loss : 0.59549\n",
      "\t Val. Loss : 0.62963\n",
      "Epoch : 09\n",
      "\tTrain Loss : 0.57439\n",
      "\t Val. Loss : 0.63638\n",
      "Epoch : 10\n",
      "\tTrain Loss : 0.56506\n",
      "\t Val. Loss : 0.63001\n",
      "Epoch : 11\n",
      "\tTrain Loss : 0.54868\n",
      "\t Val. Loss : 0.63740\n",
      "Epoch : 12\n",
      "\tTrain Loss : 0.53263\n",
      "\t Val. Loss : 0.64127\n",
      "Epoch : 13\n",
      "\tTrain Loss : 0.51682\n",
      "\t Val. Loss : 0.67401\n",
      "Epoch : 14\n",
      "\tTrain Loss : 0.49689\n",
      "\t Val. Loss : 0.66480\n",
      "Epoch : 15\n",
      "\tTrain Loss : 0.48611\n",
      "\t Val. Loss : 0.68894\n",
      "Epoch : 16\n",
      "\tTrain Loss : 0.47420\n",
      "\t Val. Loss : 0.69010\n",
      "Epoch : 17\n",
      "\tTrain Loss : 0.45635\n",
      "\t Val. Loss : 0.67690\n",
      "Epoch : 18\n",
      "\tTrain Loss : 0.46026\n",
      "\t Val. Loss : 0.67834\n",
      "Epoch : 19\n",
      "\tTrain Loss : 0.44416\n",
      "\t Val. Loss : 0.70981\n",
      "Epoch : 20\n",
      "\tTrain Loss : 0.42967\n",
      "\t Val. Loss : 0.77411\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "N_EPOCHS = 20\n",
    "\n",
    "lstm_pool_classifier = LSTMPoolingClassifier(\n",
    "    num_embeddings = len(TEXT.vocab),\n",
    "    embedding_dim = 100,\n",
    "    hidden_size = 200,\n",
    "    num_layers = 4,\n",
    "    pad_idx = PAD_IDX\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "_ = lstm_pool_classifier.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_pool_classifier.parameters())\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(\n",
    "        lstm_pool_classifier,\n",
    "        # 모델을 CoLA 데이를 이용해 학습\n",
    "        cola_train_iterator,\n",
    "        optimizer,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    valid_loss = evaluate(\n",
    "        lstm_pool_classifier,\n",
    "        cola_valid_iterator,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss : {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b1abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비굘르 위해 모델 따로 저장\n",
    "before_tuning_lstm_pool_classifier = deepcopy(lstm_pool_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a19d9",
   "metadata": {},
   "source": [
    "### - 3. 모델 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89bffad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\n",
      "\tTrain Loss : 0.76298\n",
      "\t Val. Loss : 0.70000\n",
      "Epoch : 02\n",
      "\tTrain Loss : 0.60446\n",
      "\t Val. Loss : 0.61781\n",
      "Epoch : 03\n",
      "\tTrain Loss : 0.47171\n",
      "\t Val. Loss : 0.62794\n",
      "Epoch : 04\n",
      "\tTrain Loss : 0.46750\n",
      "\t Val. Loss : 0.63935\n",
      "Epoch : 05\n",
      "\tTrain Loss : 0.39454\n",
      "\t Val. Loss : 0.75749\n",
      "Epoch : 06\n",
      "\tTrain Loss : 0.43597\n",
      "\t Val. Loss : 0.60489\n",
      "Epoch : 07\n",
      "\tTrain Loss : 0.39991\n",
      "\t Val. Loss : 0.61854\n",
      "Epoch : 08\n",
      "\tTrain Loss : 0.46557\n",
      "\t Val. Loss : 0.64095\n",
      "Epoch : 09\n",
      "\tTrain Loss : 0.44545\n",
      "\t Val. Loss : 0.65888\n",
      "Epoch : 10\n",
      "\tTrain Loss : 0.41117\n",
      "\t Val. Loss : 0.68508\n",
      "Epoch : 11\n",
      "\tTrain Loss : 0.39435\n",
      "\t Val. Loss : 0.71774\n",
      "Epoch : 12\n",
      "\tTrain Loss : 0.41486\n",
      "\t Val. Loss : 0.76167\n",
      "Epoch : 13\n",
      "\tTrain Loss : 0.39266\n",
      "\t Val. Loss : 0.75686\n",
      "Epoch : 14\n",
      "\tTrain Loss : 0.41874\n",
      "\t Val. Loss : 0.77138\n",
      "Epoch : 15\n",
      "\tTrain Loss : 0.36933\n",
      "\t Val. Loss : 0.80601\n",
      "Epoch : 16\n",
      "\tTrain Loss : 0.30590\n",
      "\t Val. Loss : 0.84819\n",
      "Epoch : 17\n",
      "\tTrain Loss : 0.40920\n",
      "\t Val. Loss : 0.89918\n",
      "Epoch : 18\n",
      "\tTrain Loss : 0.39506\n",
      "\t Val. Loss : 0.92480\n",
      "Epoch : 19\n",
      "\tTrain Loss : 0.40580\n",
      "\t Val. Loss : 0.90120\n",
      "Epoch : 20\n",
      "\tTrain Loss : 0.36060\n",
      "\t Val. Loss : 0.89704\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "N_EPOCHS = 20\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(\n",
    "        # 앞서 학습한 모델 입력\n",
    "        lstm_pool_classifier,\n",
    "        # 수능 데이터 사용\n",
    "        sat_train_iterator,\n",
    "        optimizer,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    valid_loss = evaluate(\n",
    "        lstm_pool_classifier,\n",
    "        sat_valid_iterator,\n",
    "        bce_loss_fn,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f'Epoch : {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss : {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a47399",
   "metadata": {},
   "source": [
    "### - 4. 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "924c5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning SAT Dataset Test AUROC : 0.38462\n",
      "After fine-tuning SAT Dataset Test AUROC : 0.23077\n"
     ]
    }
   ],
   "source": [
    "_ = before_tuning_lstm_pool_classifier.cpu()\n",
    "_ = lstm_pool_classifier.cpu()\n",
    "\n",
    "pool_sat_test_auroc = test(before_tuning_lstm_pool_classifier, sat_test_iterator, 'cpu')\n",
    "pool_tuned_test_auroc = test(lstm_pool_classifier, sat_test_iterator, 'cpu')\n",
    "\n",
    "print(f'Before fine-tuning SAT Dataset Test AUROC : {pool_sat_test_auroc:.5f}')\n",
    "print(f'After fine-tuning SAT Dataset Test AUROC : {pool_tuned_test_auroc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf8e77",
   "metadata": {},
   "source": [
    "### - 5. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d506b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advanced_before_tuning_model.dill', 'wb') as f:\n",
    "    model = {\n",
    "        'TEXT' : TEXT,\n",
    "        'LABEL' : LABEL,\n",
    "        'classifier' : before_tuning_lstm_pool_classifier\n",
    "    }\n",
    "    dill.dump(model, f)\n",
    "    \n",
    "with open('advanced_after_tuning_model.dill', 'wb') as f:\n",
    "    model = {\n",
    "        'TEXT' : TEXT,\n",
    "        'LABEL' : LABEL,\n",
    "        'classifier' : lstm_pool_classifier\n",
    "    }\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed4de8",
   "metadata": {},
   "source": [
    "# 6. 데모"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc9ae6",
   "metadata": {},
   "source": [
    "### - 1. 성능 비교 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4440c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path):\n",
    "    # 주어진 파일 이름으로 저장한 모델 불러오기\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "        \n",
    "    # 수능 Test 데이터셋. Test만 불러오기\n",
    "    # path는 폴더 경로 대신 불러올 파일명 입력\n",
    "    sat_test_data = TabularDataset(\n",
    "        path = f'{DATA_PATH}/sat_test.tsv',\n",
    "        format = 'tsv',\n",
    "        # 각 모델별로 정의한 Field 사용\n",
    "        fields = [\n",
    "            ('text', model['TEXT']),\n",
    "            ('label', model['LABEL'])\n",
    "        ],\n",
    "        skip_header = 1\n",
    "    )\n",
    "    \n",
    "    # 불러온 데이터로 Data Loader 생성\n",
    "    sat_test_iterator = BucketIterator(\n",
    "        sat_test_data,\n",
    "        batch_size = 8,\n",
    "        device = None,\n",
    "        sort = False,\n",
    "        shuffle = False\n",
    "    )\n",
    "    # 저장한 모델 불러오기\n",
    "    classifier = model['classifier']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_real = []\n",
    "        y_pred = []\n",
    "        classifier.eval()\n",
    "        for batch in sat_test_iterator:\n",
    "            text = batch.text\n",
    "            label = batch.label.type(torch.FloatTensor)\n",
    "            \n",
    "            output = classifier(text).flatten().cpu()\n",
    "            \n",
    "            y_real += [label]\n",
    "            y_pred += [output]\n",
    "            \n",
    "        y_real = torch.cat(y_real)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        \n",
    "    fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    \n",
    "    return auroc.round(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5be72f",
   "metadata": {},
   "source": [
    "### - 2. 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1b91361",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m file_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dill\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 앞에서 정의한 test 함수 사용\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m auroc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 모델의 성능이 좋은 순서대로 정렬\u001b[39;00m\n\u001b[1;32m     17\u001b[0m test_auroc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(model_name, auroc)]\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     36\u001b[0m label \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     40\u001b[0m y_real \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [label]\n\u001b[1;32m     41\u001b[0m y_pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [output]\n",
      "File \u001b[0;32m~/anaconda3/envs/vip/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     embed_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     output, (_, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_layer(embed_x)\n\u001b[1;32m     27\u001b[0m     last_output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/vip/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vip/lib/python3.8/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vip/lib/python3.8/site-packages/torch/nn/functional.py:2043\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2038\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
     ]
    }
   ],
   "source": [
    "# 모델들 저장한 파일명\n",
    "model_list = [\n",
    "    'baseline_model.dill',\n",
    "    'before_tuning_model.dill',\n",
    "    'after_tuning_model.dill',\n",
    "    'advanced_before_tuning_model.dill',\n",
    "    'advanced_after_tuning_model.dill'\n",
    "]\n",
    "\n",
    "test_auroc = []\n",
    "for file_name in model_list:\n",
    "    # 파일 이름의 .dill 지우고 사용\n",
    "    model_name = file_name.replace(\".dill\", \"\")\n",
    "    # 앞에서 정의한 test 함수 사용\n",
    "    auroc = test(file_name)\n",
    "    # 모델의 성능이 좋은 순서대로 정렬\n",
    "    test_auroc += [(model_name, auroc)]\n",
    "    \n",
    "# 5\n",
    "test_auroc = sorted(test_auroc, key = lambda x: x[1], reerse = True)\n",
    "for rank, (model_name, auroc) in enumerate(test_auroc):\n",
    "    print(f'Rank {rank+1} - {model_name:30} - Test AUROC : {auroc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b1eee",
   "metadata": {},
   "source": [
    "### - 3. 문제 풀이 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f59c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_problem(model_path, problem):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    TEXT = model['TEXT']\n",
    "    classifier = model['classifier']\n",
    "    \n",
    "    # 입력받은 문장에서 필요없는 기호 지우기\n",
    "    problem = list(map(lambda x: x.replace('[', '').replace(']', ''), problem))\n",
    "    \n",
    "    # 문장을 단어로 나눠줌\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in problem]\n",
    "    sentences = []\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        # 단어를 TEXT에 들어 있는 단어장을 이용해 토큰으로 변환\n",
    "        sentences.append([TEXT.vocab.stoi[word] for word in tokenized_sentence])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        classifier.eval()\n",
    "        predict = []\n",
    "        for sentence in sentences:\n",
    "            sentence = torch.LongTensor([sentence])\n",
    "            # 모델에 넣어 결과를 출력하고 저장\n",
    "            predict += [classifier(sentence).item()]\n",
    "            \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f134c75",
   "metadata": {},
   "source": [
    "### - 4. 여러 모델을 처리하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07778962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_problem_with_models(model_list, problem):\n",
    "    scores = {}\n",
    "    for file_name in model_list:\n",
    "        model_name = file_name.replace('.dill', '')\n",
    "        # 각 모델별로 predict_problem을 통해서 결과 출력\n",
    "        score = predict_problem(file_name, problem)\n",
    "        scores[model_name] = score\n",
    "        \n",
    "    score_df = pd.DataFrame(scores).T\n",
    "    score_df.columns = [f'answer_{i}_score' for i in range(1, 6)]\n",
    "    \n",
    "    selected_answer = pd.Series(\n",
    "        # 위에서 도출된 Score중 가장 작은 값이 있는 위치를 찾음 : 0~4\n",
    "        # 문제의 보기와 숫자를 맞추기 위해 +1\n",
    "        np.argmin(score_df.values, 1) + 1\n",
    "        index = score_df.index,\n",
    "        name = \"selected_answer\"\n",
    "    )\n",
    "    \n",
    "    return pd.concat([selected_answer, score_df], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4413f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0b6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6c078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da7517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
